{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "urls = ['https://www.bbc.com/portuguese/topics/ckdxnd3k619t'] #Sites que serão minerados\n",
    "\n",
    "palavras_chaves = [] #palavras_chaves que serão utilizadas para filtrar as noticias\n",
    "\n",
    "def get_soup(urls):\n",
    "    \"\"\"Função reponsável por realizar um request GET ao site e criar um objeto bs4\"\"\"\n",
    "    try:\n",
    "        req = requests.get(urls)\n",
    "        soup  = BeautifulSoup(req.text,'html.parser')\n",
    "        return soup\n",
    "    except:\n",
    "        print(\"Não foi possivel fazer o request !\")\n",
    "        \n",
    "def filtragem():\n",
    "    \"Função responsável por filtrar as noticias baseada nas palavras-chaves\"\n",
    "    lista_links = [urls]\n",
    "    for j in urls:\n",
    "    #sites que vão ser escavados\n",
    "        soup = get_soup(j)  #criação do objeto beautifulSoup\n",
    "        for k in palavras_chaves:\n",
    "            for link in soup.find_all('a'):\n",
    "              try:\n",
    "               if '' not in link.get('href'):\n",
    "                pass\n",
    "               elif (k in link.get('href')): #inserção das palavras chaves\n",
    "                lista_links.append(link.get('href'))\n",
    "              except:\n",
    "               pass\n",
    "    return list(set(lista_links))\n",
    "\n",
    "\n",
    "#titulo\n",
    "response = requests.get(urls)\n",
    "\n",
    "content = response.content\n",
    "site = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "noticia = site.find('div', attrs={'class': 'col-24 col-lg-12 sectionGrid_grid_columnOne'})\n",
    "\n",
    "titulo = noticia.find('h3', attrs={'class': 'title_element headlineSubcontent_title'})\n",
    "print(titulo.text)\n",
    "\n",
    "\n",
    "#imagens:\n",
    "response = requests.get('https://www.uol.com.br/')\n",
    "\n",
    "content = response.content\n",
    "site = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "noticia = site.find('div', attrs={'class': 'col-24 col-lg-12 sectionGrid_grid_columnOne'})\n",
    "\n",
    "titulo = noticia.find('h3', attrs={'class': 'title_element headlineSubcontent_title'})\n",
    "\n",
    "imagens = site.find_all('img')\n",
    "pics = []\n",
    "for imagem in imagens:\n",
    "    pics.append(imagem.get('src'))\n",
    "print(pics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da54f26349b3a5019c3ca4f64555589e2573eb82e1da75735a0ef0cda44c19a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
